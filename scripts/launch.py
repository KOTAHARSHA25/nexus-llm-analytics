#!/usr/bin/env python3
"""
Nexus LLM Analytics - Application Launcher

This script handles the complete startup process for Nexus LLM Analytics,
including environment setup, dependency checks, and launching both backend and frontend.
"""

import os
import sys
import subprocess
import time
import signal
import psutil
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
import argparse

# Color codes for terminal output
class Colors:
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    BOLD = '\033[1m'
    END = '\033[0m'

class NexusLauncher:
    def __init__(self):
        self.backend_process = None
        self.frontend_process = None
        self.project_root = Path(__file__).parent.parent
        
    def print_banner(self):
        """Print the application banner"""
        banner = f"""
{Colors.BOLD}{Colors.BLUE}
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                              ‚ïë
‚ïë    üöÄ NEXUS LLM ANALYTICS - Application Launcher üöÄ         ‚ïë
‚ïë                                                              ‚ïë
‚ïë    Multi-Agent Data Analysis Assistant                       ‚ïë
‚ïë    Local-First ‚Ä¢ Privacy-Preserving ‚Ä¢ AI-Powered            ‚ïë
‚ïë                                                              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
{Colors.END}
        """
        print(banner)

    def check_requirements(self):
        """Check if system meets requirements"""
        print(f"{Colors.BOLD}üîç Checking system requirements...{Colors.END}")
        
        # Check Python version
        if sys.version_info < (3, 8):
            print(f"{Colors.RED}‚ùå Python 3.8+ required. Current: {sys.version}{Colors.END}")
            return False
        
        # Check if requirements.txt exists
        requirements_file = self.project_root / "requirements.txt"
        if not requirements_file.exists():
            print(f"{Colors.RED}‚ùå requirements.txt not found{Colors.END}")
            return False
        
        # Check if main backend file exists
        main_file = self.project_root / "src" / "backend" / "main.py"
        if not main_file.exists():
            print(f"{Colors.RED}‚ùå Backend main.py not found at {main_file}{Colors.END}")
            return False
        
        # Check if frontend package.json exists
        package_json = self.project_root / "src" / "frontend" / "package.json"
        if not package_json.exists():
            print(f"{Colors.RED}‚ùå Frontend package.json not found{Colors.END}")
            return False
        
        print(f"{Colors.GREEN}‚úÖ System requirements check passed{Colors.END}")
        return True

    def _get_local_ip(self) -> str:
        """Get the local IP address for network access"""
        import socket
        try:
            # Create a socket to determine the local IP
            s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            s.connect(("8.8.8.8", 80))  # Doesn't actually send data
            local_ip = s.getsockname()[0]
            s.close()
            return local_ip
        except Exception:
            return "YOUR_IP"

    def check_ollama(self):
        """Check if Ollama is available and required models are installed"""
        print(f"{Colors.BOLD}ü§ñ Checking Ollama...{Colors.END}")
        
        try:
            result = subprocess.run(['ollama', '--version'], 
                                  capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                print(f"{Colors.GREEN}‚úÖ Ollama is installed{Colors.END}")
                
                # Check if server is running
                try:
                    result = subprocess.run(['ollama', 'list'], 
                                          capture_output=True, text=True, timeout=5)
                    if result.returncode == 0:
                        print(f"{Colors.GREEN}‚úÖ Ollama server is running{Colors.END}")
                        
                        # Check required models
                        models_output = result.stdout
                        return self.verify_required_models(models_output)
                    else:
                        print(f"{Colors.YELLOW}‚ö†Ô∏è  Ollama server not running. Starting...{Colors.END}")
                        return self.start_ollama()
                except subprocess.TimeoutExpired:
                    print(f"{Colors.YELLOW}‚ö†Ô∏è  Ollama server not responding. Starting...{Colors.END}")
                    return self.start_ollama()
            else:
                print(f"{Colors.RED}‚ùå Ollama not working properly{Colors.END}")
                return False
        except FileNotFoundError:
            print(f"{Colors.RED}‚ùå Ollama not installed. Please install from https://ollama.ai{Colors.END}")
            print(f"\n{Colors.BOLD}Installation instructions:{Colors.END}")
            print(f"  Windows: Download from https://ollama.ai/download")
            print(f"  macOS:   brew install ollama")
            print(f"  Linux:   curl https://ollama.ai/install.sh | sh")
            return False

    def verify_required_models(self, models_output):
        """Verify that required LLM models are installed"""
        print(f"{Colors.BOLD}üîç Checking required models...{Colors.END}")
        
        required_models = {
            'llama3.1:8b': 'Primary LLM (Analysis, RAG, Visualization)',
            'phi3:mini': 'Review LLM (Quality Control)',
            'nomic-embed-text': 'Embeddings (ChromaDB/RAG)'
        }
        
        missing_models = []
        installed_models = []
        
        for model_name, purpose in required_models.items():
            # Check if model appears in the list (handle different naming formats)
            model_base = model_name.split(':')[0]
            if model_base in models_output.lower() or model_name in models_output.lower():
                print(f"{Colors.GREEN}  ‚úÖ {model_name} - {purpose}{Colors.END}")
                installed_models.append(model_name)
            else:
                print(f"{Colors.RED}  ‚ùå {model_name} - {purpose} (MISSING){Colors.END}")
                missing_models.append(model_name)
        
        if missing_models:
            print(f"\n{Colors.YELLOW}‚ö†Ô∏è  Missing required models for full functionality!{Colors.END}")
            print(f"\n{Colors.BOLD}To install missing models, run:{Colors.END}")
            for model in missing_models:
                print(f"  ollama pull {model}")
            print(f"\n{Colors.YELLOW}Note: The application will start but AI features will be limited.{Colors.END}")
            
            # Ask user if they want to continue
            try:
                response = input(f"\n{Colors.BOLD}Continue anyway? (y/n): {Colors.END}").lower()
                if response != 'y':
                    print(f"{Colors.YELLOW}Exiting... Please install models and try again.{Colors.END}")
                    return False
            except KeyboardInterrupt:
                print(f"\n{Colors.YELLOW}Cancelled by user{Colors.END}")
                return False
        else:
            print(f"{Colors.GREEN}‚úÖ All required models are installed!{Colors.END}")
        
        return True

    def start_ollama(self):
        """Start Ollama server in background"""
        try:
            print(f"{Colors.YELLOW}üîÑ Starting Ollama server...{Colors.END}")
            subprocess.Popen(['ollama', 'serve'], 
                           stdout=subprocess.DEVNULL, 
                           stderr=subprocess.DEVNULL)
            
            # Wait a moment for server to start
            time.sleep(3)
            
            # Verify it's running
            result = subprocess.run(['ollama', 'list'], 
                                  capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                print(f"{Colors.GREEN}‚úÖ Ollama server started successfully{Colors.END}")
                
                # Check models after starting server
                return self.verify_required_models(result.stdout)
            else:
                print(f"{Colors.RED}‚ùå Failed to start Ollama server{Colors.END}")
                return False
        except Exception as e:
            print(f"{Colors.RED}‚ùå Error starting Ollama: {e}{Colors.END}")
            return False

    def setup_environment(self):
        """Setup environment variables"""
        print(f"{Colors.BOLD}‚öôÔ∏è  Setting up environment...{Colors.END}")
        
        env_file = self.project_root / ".env"
        env_example = self.project_root / "config" / ".env.example"
        
        if not env_file.exists() and env_example.exists():
            print(f"{Colors.YELLOW}üìÑ Creating .env from template...{Colors.END}")
            import shutil
            shutil.copy(env_example, env_file)
            print(f"{Colors.GREEN}‚úÖ .env file created{Colors.END}")
        
        return True

    def install_dependencies(self, backend_only=False):
        """Install Python and Node dependencies"""
        print(f"{Colors.BOLD}üì¶ Installing dependencies...{Colors.END}")
        
        # Install Python dependencies
        try:
            print(f"{Colors.YELLOW}üêç Installing Python dependencies...{Colors.END}")
            result = subprocess.run([sys.executable, '-m', 'pip', 'install', '-r', 'requirements.txt'],
                                  cwd=self.project_root, capture_output=True, text=True)
            if result.returncode == 0:
                print(f"{Colors.GREEN}‚úÖ Python dependencies installed{Colors.END}")
            else:
                print(f"{Colors.RED}‚ùå Failed to install Python dependencies{Colors.END}")
                print(result.stderr)
                return False
        except Exception as e:
            print(f"{Colors.RED}‚ùå Error installing Python dependencies: {e}{Colors.END}")
            return False
        
        if backend_only:
            return True
        
        # Install Node dependencies
        frontend_dir = self.project_root / "src" / "frontend"
        if frontend_dir.exists() and (frontend_dir / "package.json").exists():
            try:
                print(f"{Colors.YELLOW}üì¶ Installing Node.js dependencies...{Colors.END}")
                result = subprocess.run(['npm', 'install'], cwd=frontend_dir, 
                                      capture_output=True, text=True)
                if result.returncode == 0:
                    print(f"{Colors.GREEN}‚úÖ Node.js dependencies installed{Colors.END}")
                else:
                    print(f"{Colors.RED}‚ùå Failed to install Node.js dependencies{Colors.END}")
                    print(result.stderr)
                    return False
            except Exception as e:
                print(f"{Colors.RED}‚ùå Error installing Node.js dependencies: {e}{Colors.END}")
                return False
        
        return True

    def start_backend(self):
        """Start the FastAPI backend server"""
        print(f"{Colors.BOLD}üîß Starting backend server...{Colors.END}")
        
        # Get local IP for network access info
        local_ip = self._get_local_ip()
        
        try:
            cmd = [
                sys.executable, '-m', 'uvicorn', 
                'src.backend.main:app',
                '--reload',
                '--host', '0.0.0.0',  # Bind to all interfaces for network access
                '--port', '8000'
            ]
            
            self.backend_process = subprocess.Popen(
                cmd, cwd=self.project_root,
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE,
                text=True
            )
            
            # Wait a moment and check if it started
            time.sleep(3)
            if self.backend_process.poll() is None:
                print(f"{Colors.GREEN}‚úÖ Backend server started:{Colors.END}")
                print(f"   ‚Ä¢ Local:   http://localhost:8000")
                print(f"   ‚Ä¢ Network: http://{local_ip}:8000")
                return True
            else:
                stdout, stderr = self.backend_process.communicate()
                print(f"{Colors.RED}‚ùå Backend server failed to start{Colors.END}")
                print(f"Error: {stderr}")
                return False
                
        except Exception as e:
            print(f"{Colors.RED}‚ùå Error starting backend: {e}{Colors.END}")
            return False

    def start_frontend(self):
        """Start the Next.js frontend development server"""
        print(f"{Colors.BOLD}üé® Starting frontend development server...{Colors.END}")
        
        frontend_dir = self.project_root / "src" / "frontend"
        
        if not frontend_dir.exists():
            print(f"{Colors.YELLOW}‚ö†Ô∏è  Frontend directory not found, skipping...{Colors.END}")
            return True
        
        try:
            self.frontend_process = subprocess.Popen(
                ['npm', 'run', 'dev'],
                cwd=frontend_dir,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            # Wait a moment and check if it started
            time.sleep(5)
            if self.frontend_process.poll() is None:
                print(f"{Colors.GREEN}‚úÖ Frontend server started on http://localhost:3000{Colors.END}")
                return True
            else:
                stdout, stderr = self.frontend_process.communicate()
                print(f"{Colors.RED}‚ùå Frontend server failed to start{Colors.END}")
                print(f"Error: {stderr}")
                return False
                
        except Exception as e:
            print(f"{Colors.RED}‚ùå Error starting frontend: {e}{Colors.END}")
            return False

    def cleanup(self):
        """Clean up running processes"""
        print(f"\n{Colors.YELLOW}üßπ Cleaning up...{Colors.END}")
        
        if self.backend_process and self.backend_process.poll() is None:
            print("Stopping backend server...")
            self.backend_process.terminate()
            try:
                self.backend_process.wait(timeout=10)
            except subprocess.TimeoutExpired:
                self.backend_process.kill()
        
        if self.frontend_process and self.frontend_process.poll() is None:
            print("Stopping frontend server...")
            self.frontend_process.terminate()
            try:
                self.frontend_process.wait(timeout=10)
            except subprocess.TimeoutExpired:
                self.frontend_process.kill()
        
        print(f"{Colors.GREEN}‚úÖ Cleanup complete{Colors.END}")

    def run(self, backend_only=False, skip_deps=False, no_ollama_check=False):
        """Main run method"""
        self.print_banner()
        
        # Setup signal handler for graceful shutdown
        def signal_handler(sig, frame):
            print(f"\n{Colors.YELLOW}üì¢ Received shutdown signal...{Colors.END}")
            self.cleanup()
            sys.exit(0)
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
        
        try:
            # Run checks
            if not self.check_requirements():
                return False
            
            # Check Ollama unless explicitly skipped
            if not no_ollama_check:
                ollama_ok = self.check_ollama()
                if not ollama_ok:
                    print(f"\n{Colors.YELLOW}‚ö†Ô∏è  Continuing without Ollama (limited functionality){Colors.END}")
                    print(f"{Colors.YELLOW}   ‚Ä¢ File uploads will work{Colors.END}")
                    print(f"{Colors.YELLOW}   ‚Ä¢ API routing will work{Colors.END}")
                    print(f"{Colors.RED}   ‚Ä¢ AI analysis will NOT work (requires Ollama + models){Colors.END}")
                    print(f"{Colors.RED}   ‚Ä¢ RAG queries will NOT work{Colors.END}")
                    print(f"{Colors.RED}   ‚Ä¢ Visualizations will NOT work{Colors.END}")
                    print(f"{Colors.RED}   ‚Ä¢ Reports will NOT work{Colors.END}")
            else:
                print(f"{Colors.YELLOW}‚ö†Ô∏è  Skipping Ollama check (--no-ollama-check flag){Colors.END}")
            
            if not self.setup_environment():
                return False
            
            if not skip_deps and not self.install_dependencies(backend_only):
                print(f"{Colors.RED}‚ùå Dependency installation failed{Colors.END}")
                return False
            
            # Start services
            if not self.start_backend():
                return False
            
            if not backend_only and not self.start_frontend():
                self.cleanup()
                return False
            
            # Print success message
            local_ip = self._get_local_ip()
            print(f"\n{Colors.BOLD}{Colors.GREEN}üéâ Nexus LLM Analytics is running!{Colors.END}")
            print(f"\n{Colors.BOLD}üåê Access your application at:{Colors.END}")
            print(f"   ‚Ä¢ Backend API (Local):   http://localhost:8000")
            print(f"   ‚Ä¢ Backend API (Network): http://{local_ip}:8000")
            if not backend_only:
                print(f"   ‚Ä¢ Frontend UI: http://localhost:3000")
            print(f"   ‚Ä¢ API Docs: http://localhost:8000/docs")
            print(f"\n{Colors.BOLD}üì± Other devices on same WiFi can connect using:{Colors.END}")
            print(f"   ‚Ä¢ Backend URL: http://{local_ip}:8000")
            
            print(f"\n{Colors.BOLD}üìã Available endpoints:{Colors.END}")
            print(f"   ‚Ä¢ POST /analyze/ - Analyze data with natural language")
            print(f"   ‚Ä¢ POST /upload-documents/ - Upload data files")
            print(f"   ‚Ä¢ POST /generate-report/ - Generate reports")
            print(f"   ‚Ä¢ POST /visualize/generate - Create visualizations")
            
            print(f"\n{Colors.BOLD}‚ö° Tech Stack Running:{Colors.END}")
            print(f"   ‚Ä¢ LLM: Llama 3.1 8B (Primary) + Phi-3-mini (Review)")
            print(f"   ‚Ä¢ Framework: CrewAI Multi-Agent System")
            print(f"   ‚Ä¢ Backend: FastAPI + Uvicorn")
            print(f"   ‚Ä¢ Frontend: Next.js 14 + React 18")
            print(f"   ‚Ä¢ Vector DB: ChromaDB")
            print(f"   ‚Ä¢ Data: Pandas + Polars")
            print(f"   ‚Ä¢ Viz: Plotly + Recharts")
            print(f"   ‚Ä¢ Reports: OpenPyXL + ReportLab")
            
            print(f"\n{Colors.BOLD}üí° Quick Start:{Colors.END}")
            print(f"   1. Upload a file (CSV, JSON, PDF, TXT)")
            print(f"   2. Ask natural language questions")
            print(f"   3. Get AI-powered analysis, charts, and reports")
            
            print(f"\n{Colors.BOLD}üìå Important Notes:{Colors.END}")
            print(f"   ‚Ä¢ Ollama must be running for AI features")
            print(f"   ‚Ä¢ All processing is 100% local (private)")
            print(f"   ‚Ä¢ Files stored in: data/samples/")
            print(f"   ‚Ä¢ Reports saved in: reports/")
            
            print(f"\n{Colors.YELLOW}Press Ctrl+C to stop all services{Colors.END}")
            
            # Keep running until interrupted
            try:
                while True:
                    # Check if processes are still running
                    if self.backend_process and self.backend_process.poll() is not None:
                        print(f"{Colors.RED}‚ùå Backend process stopped unexpectedly{Colors.END}")
                        break
                    
                    if not backend_only and self.frontend_process and self.frontend_process.poll() is not None:
                        print(f"{Colors.RED}‚ùå Frontend process stopped unexpectedly{Colors.END}")
                        break
                    
                    time.sleep(1)
            except KeyboardInterrupt:
                pass
            
        except Exception as e:
            print(f"{Colors.RED}‚ùå Unexpected error: {e}{Colors.END}")
            return False
        finally:
            self.cleanup()
        
        return True

def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(description='Nexus LLM Analytics Launcher')
    parser.add_argument('--backend-only', action='store_true', 
                       help='Start only the backend server')
    parser.add_argument('--skip-deps', action='store_true',
                       help='Skip dependency installation')
    parser.add_argument('--no-ollama-check', action='store_true',
                       help='Skip Ollama verification (for testing without AI features)')
    
    args = parser.parse_args()
    
    launcher = NexusLauncher()
    success = launcher.run(
        backend_only=args.backend_only, 
        skip_deps=args.skip_deps,
        no_ollama_check=args.no_ollama_check
    )
    
    if success:
        print(f"\n{Colors.GREEN}üëã Thanks for using Nexus LLM Analytics!{Colors.END}")
    else:
        print(f"\n{Colors.RED}‚ùå Startup failed. Check the errors above.{Colors.END}")
        sys.exit(1)

if __name__ == "__main__":
    main()