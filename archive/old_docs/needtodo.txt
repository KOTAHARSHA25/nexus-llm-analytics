# ğŸ“Š Nexus LLM Analytics - Data Flow Guide

## Quick Overview: How Different Data Types Flow Through the System

---

### 1. ğŸ“„ **PDF/Document Flow** (RAG - Retrieval Augmented Generation)

```
User uploads Test.pdf
    â†“
Frontend â†’ POST /upload-documents/
    â†“
Backend extracts text (PyPDF2/python-docx)
    â†“
Text â†’ ChromaDB (Vector Database)
    â†“ (stores embeddings with nomic-embed-text)
User asks: "What are the main topics?"
    â†“
Frontend â†’ POST /analyze/
    â†“
crew_manager.py detects file type: .pdf
    â†“
Routes to: RAG Agent (rag_agent.py)
    â†“
RAG Agent:
  1. Searches ChromaDB for relevant chunks
  2. Retrieves top 5 matching sections
  3. Sends to phi3:mini: "Based on this content: [chunks], answer: What are the main topics?"
    â†“
phi3:mini analyzes and generates response
    â†“
Result sent to frontend â†’ Display in Results tab
```

**Key Files:**
- rag_agent.py - RAG processing
- optimized_tools.py - ChromaDB queries
- `src/backend/agents/crew_manager.py:587-680` - Direct RAG execution

---

### 2. ğŸ“Š **CSV/Excel Flow** (Structured Data Analysis)

```
User uploads StressLevelDataset.csv
    â†“
Frontend â†’ POST /upload-documents/
    â†“
Backend reads with pandas
    â†“
Stored in: data/uploads/StressLevelDataset.csv
    â†“
User asks: "What is the average stress level?"
    â†“
Frontend â†’ POST /analyze/
    â†“
crew_manager.py detects file type: .csv
    â†“
Routes based on query + confidence scores:
  â€¢ Statistical Agent (0.85) â† WINNER
  â€¢ Financial Agent (0.40)
  â€¢ ML Insights (0.30)
    â†“
Statistical Agent (statistical_agent.py):
  1. Loads CSV into pandas DataFrame
  2. Calculates: mean, median, std, correlation
  3. Generates code: df['stress_level'].mean()
  4. Executes in sandbox
    â†“
phi3:mini formats results into natural language
    â†“
Result sent to frontend â†’ Display with charts
```

**Key Files:**
- statistical_agent.py - Statistical analysis
- financial_agent.py - Revenue/profit analysis
- ml_insights_agent.py - ML predictions
- data_utils.py - CSV reading

---

### 3. ğŸ“ˆ **Time Series Flow** (Date-based Data)

```
User uploads sales_data.csv (with date column)
    â†“
Backend detects datetime column
    â†“
User asks: "Show me the trend over time"
    â†“
Routes to: Time Series Agent (time_series_agent.py)
    â†“
Time Series Agent:
  1. Converts date column â†’ datetime
  2. Sets as index
  3. Detects seasonality (autocorrelation)
  4. Calculates trend (linear regression)
  5. Forecasts next period
    â†“
phi3:mini explains findings
    â†“
Frontend displays trend chart + forecast
```

**Key Files:**
- time_series_agent.py - Time series analysis
- visualize.py - Chart generation

---

### 4. ğŸ’° **Financial Data Flow**

```
User uploads financial_report.csv
  (columns: revenue, expenses, profit, date)
    â†“
User asks: "What is the profit margin?"
    â†“
Routes to: Financial Agent (financial_agent.py)
    â†“
Financial Agent:
  1. Identifies financial columns (revenue, profit, margin)
  2. Calculates: gross_margin = (revenue - cost) / revenue * 100
  3. Calculates: growth_rate = (current - previous) / previous * 100
  4. Benchmarks against standards
    â†“
phi3:mini generates financial report
    â†“
Frontend displays metrics + recommendations
```

**Key Files:**
- financial_agent.py - Financial calculations
- enhanced_reports.py - PDF report generation

---

### 5. ğŸ¤– **ML Predictions Flow**

```
User uploads customer_data.csv
    â†“
User asks: "Predict customer churn"
    â†“
Routes to: ML Insights Agent (ml_insights_agent.py)
    â†“
ML Agent:
  1. Preprocesses data (scaling, encoding)
  2. Splits train/test (80/20)
  3. Trains model (Random Forest)
  4. Predicts on test set
  5. Calculates metrics (accuracy, precision)
    â†“
phi3:mini explains predictions + feature importance
    â†“
Frontend displays model performance + predictions
```

**Key Files:**
- ml_insights_agent.py - ML training
- security_guards.py - Safe code execution

---

### 6. ğŸ”„ **Review Protocol Flow** (Two Models Working Together)

```
User asks complex question
    â†“
PRIMARY MODEL (phi3:mini) analyzes
    â†“
Result: "The average is 75.3"
    â†“
execute_with_review_protocol() activates
    â†“
REVIEW MODEL (phi3:mini or tinyllama) checks:
  âœ“ Is calculation correct?
  âœ“ Any missing insights?
  âœ“ Alternative interpretations?
    â†“
Review feedback: "Consider also showing median (72.1) due to outliers"
    â†“
PRIMARY refines: "Average is 75.3, but median of 72.1 is more representative due to outliers"
    â†“ (max 3 rounds)
Final result â†’ Frontend
```

**Key Files:**
- `src/backend/agents/crew_manager.py:181-360` - Review protocol
- `src/backend/api/analyze.py:167-247` - Review endpoint

---

## ğŸ¯ Complete Request Flow (Any Data Type)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. FRONTEND (Next.js)                                       â”‚
â”‚    User uploads file + asks question                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. BACKEND API (FastAPI)                                    â”‚
â”‚    â€¢ POST /upload-documents/ â†’ Save file                    â”‚
â”‚    â€¢ POST /analyze/ â†’ Start analysis                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. CREW MANAGER (crew_manager.py)                           â”‚
â”‚    â€¢ Detects file type: .pdf, .csv, .xlsx, .json           â”‚
â”‚    â€¢ Calculates confidence scores for each agent            â”‚
â”‚    â€¢ Routes to best agent (Statistical, RAG, Financial...)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. SPECIALIZED AGENT (Plugin)                               â”‚
â”‚    â€¢ RAG Agent â†’ Document Q&A                               â”‚
â”‚    â€¢ Statistical Agent â†’ Data analysis                      â”‚
â”‚    â€¢ Financial Agent â†’ Revenue/profit                       â”‚
â”‚    â€¢ ML Agent â†’ Predictions                                 â”‚
â”‚    â€¢ Time Series â†’ Trends/forecasts                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. LLM CLIENT (llm_client.py)                               â”‚
â”‚    Sends to Ollama:                                         â”‚
â”‚    â€¢ Primary Model: phi3:mini (analysis)                    â”‚
â”‚    â€¢ Review Model: phi3:mini (quality check)                â”‚
â”‚    â€¢ Embedding: nomic-embed-text (RAG)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. RESULT PROCESSING                                        â”‚
â”‚    â€¢ Format response                                        â”‚
â”‚    â€¢ Generate visualizations (charts)                       â”‚
â”‚    â€¢ Create reports (PDF/Excel)                             â”‚
â”‚    â€¢ Save to history                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. FRONTEND DISPLAY                                         â”‚
â”‚    â€¢ Results tab â†’ Analysis text                            â”‚
â”‚    â€¢ Insights tab â†’ Review insights                         â”‚
â”‚    â€¢ Visualizations â†’ Charts                                â”‚
â”‚    â€¢ Download â†’ Export reports                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Key Directories

```
src/backend/
â”œâ”€â”€ api/              # HTTP endpoints
â”‚   â”œâ”€â”€ analyze.py    # Main analysis endpoint
â”‚   â”œâ”€â”€ upload.py     # File upload
â”‚   â””â”€â”€ models.py     # Model configuration
â”œâ”€â”€ agents/           # Core orchestration
â”‚   â”œâ”€â”€ crew_manager.py    # Routes to agents
â”‚   â””â”€â”€ rag_agent.py       # RAG specialist
â”œâ”€â”€ plugins/          # Specialized agents
â”‚   â”œâ”€â”€ statistical_agent.py
â”‚   â”œâ”€â”€ financial_agent.py
â”‚   â”œâ”€â”€ ml_insights_agent.py
â”‚   â””â”€â”€ time_series_agent.py
â”œâ”€â”€ core/             # Core utilities
â”‚   â”œâ”€â”€ llm_client.py      # Ollama connection
â”‚   â””â”€â”€ optimized_tools.py # ChromaDB RAG
â””â”€â”€ data/
    â””â”€â”€ uploads/      # Uploaded files stored here
```

---

## ğŸ”‘ Key Takeaways

1. **File Type Detection**: Automatic routing based on extension
2. **Confidence Scoring**: Each agent scores query relevance (0.0-1.0)
3. **Dynamic Models**: User can change models anytime in settings
4. **Review Protocol**: Two models collaborate for better results
5. **Safe Execution**: Sandboxed code execution for data analysis

Need more details on any specific flow? Just ask! ğŸš€




This is a very robust and well-designed API for visualization, especially with the LIDA-inspired goal-based approach and the deterministic chart generation. However, there are always areas for improvement and expansion. Here are some changes or features that could make it even better:

Core Functionality & Robustness:

More Comprehensive Data Preprocessing/Cleaning:

Missing Value Handling: Options for imputation (mean, median, mode), dropping rows/columns, or visualizing missingness.

Outlier Detection/Handling: Simple methods like IQR or Z-score based outlier flagging, or even visualization of outliers.

Data Type Inference Improvement: While Pandas does a good job, sometimes numeric columns are read as objects (e.g., due to commas, currency symbols). A more aggressive type inference and cleaning step could be beneficial.

Feature Engineering: Basic feature creation, like extracting year/month/day from datetime columns or creating simple ratios.

Advanced ChartTypeAnalyzer and DynamicChartGenerator:

Support for More Chart Types: Heatmaps, violin plots, boxen plots, area charts, stack bar charts, multi-line charts (faceted/grouped), treemaps, sunbursts.

Statistical Charts: Regression plots, correlation matrices, QQ plots, residual plots for more in-depth analysis.

Geospatial Visualization: If the data contains location information (lat/lon, city names), integrating libraries like Plotly's px.scatter_mapbox or folium would be powerful.

Contextual Chart Parameters: Beyond x_col, y_col, consider color_col, size_col, facet_col, animation_frame for richer visualizations. The analyzer could suggest these based on the data.

Handling Time Series Data: Specific suggestions for time series charts (e.g., resampling, rolling averages) if a datetime_column is detected.

Enhanced Error Handling and User Feedback:

Specific Error Messages: Instead of just "Chart generation failed," provide more granular details, e.g., "Column 'X' not found," "Insufficient numeric data for scatter plot."

Pre-computation Checks: Before attempting to generate a chart, perform checks (e.g., "Is x_col numeric for a histogram?").

Warnings for Suboptimal Choices: If the LLM or auto-selection makes a choice that might be technically valid but visually suboptimal, log a warning (e.g., "Pie chart for 20 categories might be hard to read").

Security for exec() and eval():

While the execute_plotly_code uses exec() and notes that Plotly modules can't pass the sandbox, it's a known security risk. For a production system, carefully consider the implications. If user-provided code is executed, ensure strong sanitization and resource limits (e.g., time limits, memory limits) to prevent malicious code or denial-of-service attacks. The EnhancedSandbox is a good start, but its bypass for Plotly is a consideration.

User Experience & AI Integration:

Interactive Goal Refinement:

If the initial goal or auto-selection doesn't yield a great chart, the system could proactively ask clarifying questions or offer alternatives.

"Would you like to see the distribution of sales by product category instead of region?"

"I noticed a Date column; would you like to see sales trend over time?"

"Explain the Chart" Feature:

After generating a chart, use an LLM to generate a textual explanation of what the chart shows, key insights, and what columns were used. This is a common feature in modern BI tools.

"Improve this Chart" / "Change Chart Type":

Allow users to request changes like "make this a bar chart," "change the x-axis to 'Product'," "add 'Region' as a color dimension." This would feed directly back into the DynamicChartGenerator.

Multi-Chart Dashboards/Comparison:

Enable generation of multiple related charts or even simple dashboards from a single, more complex goal (e.g., "Show sales distribution by region and sales trend over time").

Persistent Storage/Sharing of Visualizations:

Saving generated figure_json to a database or cloud storage, along with metadata (goal, data used, parameters), for later retrieval and sharing.

Dataset Previews/Metadata:

In the /goal-based and /suggestions endpoints, returning more detailed metadata about the columns (e.g., min/max for numeric, top 5 categories for categorical, count of unique values) would enhance the user's understanding and the LLM's ability to make better decisions.

Technical & Architectural:

Asynchronous Processing for Heavy Visualizations:

For very large datasets or complex chart types, visualization generation can be time-consuming. Consider using background tasks (e.g., with Celery/Redis, or FastAPI's BackgroundTasks) to prevent API timeouts and provide a better user experience.

Frontend Integration/Client Libraries:

While this is a backend API, thinking about how a frontend would consume it (e.g., with a React/Vue component that renders Plotly JSON) could influence the API's structure.

Versioning the API:

As features are added, API versioning (/v1/generate, /v2/generate) is good practice to prevent breaking changes for existing clients.

Containerization (Docker):

Ensuring the entire backend (FastAPI, CrewAI, all dependencies) is easily deployable via Docker for consistency across environments.

Example Scenario for Improvement:

Imagine a user types: "Show me the relationship between 'price' and 'profit' for 'electronics' products, colored by 'region'."

Current system might:

Filter for 'electronics' (if LLM is good).

Identify 'price' and 'profit' as numeric.

Suggest a scatter plot.

Generate a scatter plot of 'price' vs 'profit'.

Improved system could:

Filter for 'electronics'.

Identify 'price' (x), 'profit' (y) and 'region' (color).

Suggest and generate a scatter plot with points colored by region, possibly even faceting by region or adding tooltips with more details.

Provide a natural language summary: "This scatter plot shows the relationship between product price and profit for electronics. You can observe the distribution of profits across different price points, with points colored to distinguish between regions. For example, products in North America tend to have higher prices and profits compared to those in Asia."

By adding more nuanced understanding of user intent and richer chart parameters, the system can go from generating a "correct" chart to generating the "most insightful" chart.