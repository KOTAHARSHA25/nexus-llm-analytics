"""
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
NEXUS LLM ANALYTICS - BENCHMARK RUNNER
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Phase 4.1: Comprehensive benchmark execution using the 160-query dataset.

Features:
- Loads benchmark_dataset.json
- Executes queries against the system
- Collects metrics (latency, accuracy, model selection)
- Generates research-grade reports

Usage:
    python benchmarks/benchmark_runner.py --mode quick     # 20 queries
    python benchmarks/benchmark_runner.py --mode standard  # 80 queries  
    python benchmarks/benchmark_runner.py --mode full      # All 160 queries

Version: 1.0.0
"""

import sys
import json
import time
import random
import argparse
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, field, asdict
from typing import Dict, Any, List, Optional, Tuple
from collections import defaultdict

# Setup path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "src"))


@dataclass
class QueryResult:
    """Result of a single benchmark query"""
    query_id: str
    domain: str
    query: str
    complexity: str
    query_type: str
    expected_model: str
    actual_model: str
    model_correct: bool
    success: bool
    latency_seconds: float
    iterations: int
    review_applied: bool
    response_length: int
    required_elements_found: int
    required_elements_total: int
    completeness_score: float
    error: Optional[str] = None
    
    def to_dict(self) -> dict:
        return asdict(self)


@dataclass
class BenchmarkReport:
    """Complete benchmark report"""
    timestamp: str
    mode: str
    total_queries: int
    successful_queries: int
    failed_queries: int
    success_rate: float
    avg_latency_seconds: float
    model_selection_accuracy: float
    avg_completeness_score: float
    avg_iterations: float
    review_rate: float
    results_by_domain: Dict[str, Dict]
    results_by_complexity: Dict[str, Dict]
    results_by_type: Dict[str, Dict]
    detailed_results: List[Dict]


class BenchmarkRunner:
    """
    Executes benchmark queries and collects research metrics.
    """
    
    def __init__(self, dataset_path: Optional[Path] = None, mode: str = "standard", simulate: bool = False):
        self.dataset_path = dataset_path or (project_root / "benchmarks" / "benchmark_dataset.json")
        self.mode = mode
        self.simulate = simulate
        self.dataset = self._load_dataset()
        self.results: List[QueryResult] = []
        
        # Import system components
        if not simulate:
            self._init_system()
        else:
            self.system_available = False
    
    def _load_dataset(self) -> dict:
        """Load the benchmark dataset"""
        with open(self.dataset_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _init_system(self):
        """Initialize system components"""
        try:
            from backend.core.engine.query_orchestrator import QueryOrchestrator
            from backend.core.llm_client import LLMClient
            
            self.llm_client = LLMClient()
            self.orchestrator = QueryOrchestrator(None, {})
            self.system_available = True
            print("‚úÖ System components initialized")
        except Exception as e:
            print(f"‚ö†Ô∏è System initialization warning: {e}")
            self.system_available = False
    
    def _get_queries(self, mode: str = "standard") -> List[Dict]:
        """Get queries based on execution mode"""
        all_queries = []
        
        # Collect domain queries
        for domain_name, domain_data in self.dataset.get("domains", {}).items():
            for query in domain_data.get("queries", []):
                query["domain"] = domain_name
                all_queries.append(query)
        
        # Add cross-domain queries
        for query in self.dataset.get("cross_domain_queries", []):
            query["domain"] = "Cross_Domain"
            all_queries.append(query)
        
        # Add edge cases (only in full mode)
        if mode == "full":
            for query in self.dataset.get("edge_case_queries", []):
                query["domain"] = "Edge_Case"
                if "query" in query and query["query"]:  # Skip empty queries
                    all_queries.append(query)
        
        # Sample based on mode
        if mode == "quick":
            # 20 queries: balanced across domains and complexity
            sampled = self._stratified_sample(all_queries, 20)
        elif mode == "standard":
            # 80 queries: good coverage
            sampled = self._stratified_sample(all_queries, 80)
        else:
            # Full: all queries
            sampled = all_queries
        
        return sampled
    
    def _stratified_sample(self, queries: List[Dict], n: int) -> List[Dict]:
        """Stratified sampling to maintain diversity"""
        by_complexity = defaultdict(list)
        for q in queries:
            by_complexity[q.get("complexity", "medium")].append(q)
        
        sampled = []
        per_stratum = max(1, n // len(by_complexity))
        
        for complexity, qs in by_complexity.items():
            sample_size = min(len(qs), per_stratum)
            sampled.extend(random.sample(qs, sample_size))
        
        # Fill remaining slots
        remaining = n - len(sampled)
        if remaining > 0:
            available = [q for q in queries if q not in sampled]
            sampled.extend(random.sample(available, min(remaining, len(available))))
        
        return sampled[:n]
    
    def _execute_query(self, query_config: Dict) -> QueryResult:
        """Execute a single query and measure metrics"""
        query_id = query_config.get("id", "UNKNOWN")
        query = query_config.get("query", "")
        domain = query_config.get("domain", "Unknown")
        complexity = query_config.get("complexity", "medium")
        query_type = query_config.get("query_type", "analytical")
        expected_model = query_config.get("expected_model", "")
        required_elements = query_config.get("required_elements", [])
        
        print(f"  üìä {query_id}: {query[:50]}...")
        
        start_time = time.time()
        
        try:
            if not self.system_available:
                # Simulated execution for testing without LLM
                return self._simulate_execution(query_config, start_time)
            
            # Create execution plan
            plan = self.orchestrator.create_execution_plan(query, {"data": True})
            actual_model = plan.model
            model_correct = self._check_model_match(expected_model, actual_model)
            
            # Execute query
            response = self.llm_client.generate(
                prompt=f"Analyze this query: {query}",
                model=actual_model,
                adaptive_timeout=True
            )
            
            latency = time.time() - start_time
            
            if response.get("success"):
                response_text = response.get("response", "")
                elements_found = self._count_required_elements(response_text, required_elements)
                completeness = elements_found / max(1, len(required_elements))
                
                return QueryResult(
                    query_id=query_id,
                    domain=domain,
                    query=query[:100],
                    complexity=complexity,
                    query_type=query_type,
                    expected_model=expected_model,
                    actual_model=actual_model,
                    model_correct=model_correct,
                    success=True,
                    latency_seconds=latency,
                    iterations=1,
                    review_applied=plan.review_level.value != "none",
                    response_length=len(response_text),
                    required_elements_found=elements_found,
                    required_elements_total=len(required_elements),
                    completeness_score=completeness
                )
            else:
                raise Exception(response.get("error", "Unknown error"))
                
        except Exception as e:
            latency = time.time() - start_time
            return QueryResult(
                query_id=query_id,
                domain=domain,
                query=query[:100],
                complexity=complexity,
                query_type=query_type,
                expected_model=expected_model,
                actual_model="N/A",
                model_correct=False,
                success=False,
                latency_seconds=latency,
                iterations=0,
                review_applied=False,
                response_length=0,
                required_elements_found=0,
                required_elements_total=len(required_elements),
                completeness_score=0.0,
                error=str(e)[:200]
            )
    
    def _simulate_execution(self, query_config: Dict, start_time: float) -> QueryResult:
        """Simulate query execution for testing"""
        # Simulate processing time based on complexity
        complexity = query_config.get("complexity", "medium")
        if complexity == "simple":
            time.sleep(0.1)
        elif complexity == "medium":
            time.sleep(0.2)
        else:
            time.sleep(0.3)
        
        latency = time.time() - start_time
        expected_model = query_config.get("expected_model", "")
        required_elements = query_config.get("required_elements", [])
        
        # Simulate 85% success rate
        success = random.random() < 0.85
        completeness = random.uniform(0.7, 1.0) if success else 0.0
        elements_found = int(len(required_elements) * completeness)
        
        return QueryResult(
            query_id=query_config.get("id", "UNKNOWN"),
            domain=query_config.get("domain", "Unknown"),
            query=query_config.get("query", "")[:100],
            complexity=complexity,
            query_type=query_config.get("query_type", "analytical"),
            expected_model=expected_model,
            actual_model=expected_model,  # Simulated correct selection
            model_correct=True,
            success=success,
            latency_seconds=latency,
            iterations=2 if complexity != "simple" else 1,
            review_applied=complexity != "simple",
            response_length=random.randint(200, 2000),
            required_elements_found=elements_found,
            required_elements_total=len(required_elements),
            completeness_score=completeness,
            error=None if success else "Simulated failure"
        )
    
    def _check_model_match(self, expected: str, actual: str) -> bool:
        """Check if model selection matches expectation"""
        if not expected:
            return True
        # Normalize model names for comparison
        expected_normalized = expected.lower().replace(":", "").replace("-", "")
        actual_normalized = actual.lower().replace(":", "").replace("-", "")
        return expected_normalized in actual_normalized or actual_normalized in expected_normalized
    
    def _count_required_elements(self, text: str, required: List[str]) -> int:
        """Count how many required elements appear in the response"""
        text_lower = text.lower()
        return sum(1 for elem in required if elem.lower() in text_lower)
    
    def run(self, mode: str = "standard") -> BenchmarkReport:
        """Run the benchmark suite"""
        print(f"\n{'='*60}")
        print(f"üöÄ NEXUS BENCHMARK RUNNER - {mode.upper()} MODE")
        print(f"{'='*60}\n")
        
        queries = self._get_queries(mode)
        total = len(queries)
        print(f"üìã Loaded {total} queries for benchmark\n")
        
        self.results = []
        
        for i, query_config in enumerate(queries, 1):
            print(f"[{i}/{total}]", end="")
            result = self._execute_query(query_config)
            self.results.append(result)
            status = "‚úÖ" if result.success else "‚ùå"
            print(f" {status} ({result.latency_seconds:.2f}s)")
        
        return self._generate_report(mode)
    
    def _generate_report(self, mode: str) -> BenchmarkReport:
        """Generate comprehensive benchmark report"""
        successful = [r for r in self.results if r.success]
        failed = [r for r in self.results if not r.success]
        
        # Calculate metrics
        success_rate = len(successful) / max(1, len(self.results))
        avg_latency = sum(r.latency_seconds for r in self.results) / max(1, len(self.results))
        model_correct = sum(1 for r in self.results if r.model_correct)
        model_accuracy = model_correct / max(1, len(self.results))
        avg_completeness = sum(r.completeness_score for r in successful) / max(1, len(successful))
        avg_iterations = sum(r.iterations for r in self.results) / max(1, len(self.results))
        review_applied = sum(1 for r in self.results if r.review_applied)
        review_rate = review_applied / max(1, len(self.results))
        
        # Group by domain
        by_domain = defaultdict(list)
        for r in self.results:
            by_domain[r.domain].append(r)
        
        results_by_domain = {}
        for domain, results in by_domain.items():
            successful_domain = [r for r in results if r.success]
            results_by_domain[domain] = {
                "total": len(results),
                "success_rate": len(successful_domain) / max(1, len(results)),
                "avg_latency": sum(r.latency_seconds for r in results) / max(1, len(results)),
                "avg_completeness": sum(r.completeness_score for r in successful_domain) / max(1, len(successful_domain))
            }
        
        # Group by complexity
        by_complexity = defaultdict(list)
        for r in self.results:
            by_complexity[r.complexity].append(r)
        
        results_by_complexity = {}
        for complexity, results in by_complexity.items():
            successful_comp = [r for r in results if r.success]
            results_by_complexity[complexity] = {
                "total": len(results),
                "success_rate": len(successful_comp) / max(1, len(results)),
                "avg_latency": sum(r.latency_seconds for r in results) / max(1, len(results)),
                "model_accuracy": sum(1 for r in results if r.model_correct) / max(1, len(results))
            }
        
        # Group by query type
        by_type = defaultdict(list)
        for r in self.results:
            by_type[r.query_type].append(r)
        
        results_by_type = {}
        for qtype, results in by_type.items():
            successful_type = [r for r in results if r.success]
            results_by_type[qtype] = {
                "total": len(results),
                "success_rate": len(successful_type) / max(1, len(results)),
                "avg_latency": sum(r.latency_seconds for r in results) / max(1, len(results))
            }
        
        report = BenchmarkReport(
            timestamp=datetime.now().isoformat(),
            mode=mode,
            total_queries=len(self.results),
            successful_queries=len(successful),
            failed_queries=len(failed),
            success_rate=round(success_rate * 100, 2),
            avg_latency_seconds=round(avg_latency, 3),
            model_selection_accuracy=round(model_accuracy * 100, 2),
            avg_completeness_score=round(avg_completeness * 100, 2),
            avg_iterations=round(avg_iterations, 2),
            review_rate=round(review_rate * 100, 2),
            results_by_domain=results_by_domain,
            results_by_complexity=results_by_complexity,
            results_by_type=results_by_type,
            detailed_results=[r.to_dict() for r in self.results]
        )
        
        self._print_report(report)
        self._save_report(report)
        
        return report
    
    def _print_report(self, report: BenchmarkReport):
        """Print report to console"""
        print(f"\n{'='*60}")
        print("üìä BENCHMARK RESULTS")
        print(f"{'='*60}\n")
        
        print(f"Mode: {report.mode}")
        print(f"Timestamp: {report.timestamp}\n")
        
        print("‚îÄ" * 40)
        print("OVERALL METRICS")
        print("‚îÄ" * 40)
        print(f"  Total Queries:           {report.total_queries}")
        print(f"  ‚úÖ Successful:           {report.successful_queries}")
        print(f"  ‚ùå Failed:               {report.failed_queries}")
        print(f"  üìà Success Rate:         {report.success_rate}%")
        print(f"  ‚è±Ô∏è Avg Latency:           {report.avg_latency_seconds}s")
        print(f"  üéØ Model Selection:      {report.model_selection_accuracy}%")
        print(f"  üìã Completeness:         {report.avg_completeness_score}%")
        print(f"  üîÑ Avg Iterations:       {report.avg_iterations}")
        print(f"  üëÅÔ∏è Review Rate:          {report.review_rate}%")
        
        print("\n" + "‚îÄ" * 40)
        print("BY COMPLEXITY")
        print("‚îÄ" * 40)
        for complexity, data in report.results_by_complexity.items():
            print(f"  {complexity.upper():12} | n={data['total']:3} | success={data['success_rate']*100:.1f}% | latency={data['avg_latency']:.2f}s")
        
        print("\n" + "‚îÄ" * 40)
        print("BY DOMAIN")
        print("‚îÄ" * 40)
        for domain, data in report.results_by_domain.items():
            print(f"  {domain[:15]:15} | n={data['total']:3} | success={data['success_rate']*100:.1f}%")
        
        print("\n" + "=" * 60)
    
    def _save_report(self, report: BenchmarkReport):
        """Save report to JSON file"""
        output_dir = project_root / "benchmarks" / "results"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"benchmark_report_{report.mode}_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(report), f, indent=2, default=str)
        
        print(f"\nüíæ Report saved to: {output_file}")


def main():
    parser = argparse.ArgumentParser(description="Run Nexus LLM Analytics benchmarks")
    parser.add_argument(
        "--mode",
        choices=["quick", "standard", "full"],
        default="standard",
        help="Benchmark mode: quick (20), standard (80), full (160 queries)"
    )
    parser.add_argument(
        "--simulate",
        action="store_true",
        help="Run in simulation mode (no actual LLM calls)"
    )
    
    args = parser.parse_args()
    
    runner = BenchmarkRunner()
    
    if args.simulate:
        runner.system_available = False
        print("‚ö†Ô∏è Running in SIMULATION mode (no actual LLM calls)\n")
    
    runner.run(args.mode)


if __name__ == "__main__":
    main()
